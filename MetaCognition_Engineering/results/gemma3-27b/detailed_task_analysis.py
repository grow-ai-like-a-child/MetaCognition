#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯¦ç»†åˆ†æGemma3-27Båœ¨ä¸‰ç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½å·®å¼‚
"""

import json
from collections import defaultdict
from pathlib import Path

def detailed_task_analysis():
    """
    æŒ‰ä»»åŠ¡ç±»å‹è¯¦ç»†åˆ†ææ€§èƒ½
    """
    # åŠ è½½åˆ†æç»“æœ
    comparison_file = Path("gemma3_comparison_with_ground_truth.json")
    with open(comparison_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print("=" * 80)
    print("ğŸ” Gemma3-27B è¯¦ç»†ä»»åŠ¡åˆ†ææŠ¥å‘Š")
    print("=" * 80)
    
    # æŒ‰ä»»åŠ¡åˆ†ç»„
    tasks = {}
    for item in data:
        task = item['task']
        if task not in tasks:
            tasks[task] = []
        tasks[task].append(item)
    
    # æ€»ä½“ç»Ÿè®¡
    total_correct = sum(1 for item in data if item['is_correct'])
    total_count = len(data)
    overall_accuracy = total_correct / total_count * 100
    
    print(f"\nğŸ“Š æ€»ä½“æ¦‚è§ˆ:")
    print(f"   æ€»å‡†ç¡®ç‡: {overall_accuracy:.2f}% ({total_correct}/{total_count})")
    print(f"   æ•°æ®åˆ†å¸ƒ: Grid={len(tasks.get('Grid', []))}, Color={len(tasks.get('Color', []))}, Gabor={len(tasks.get('Gabor', []))}")
    
    # æ¯ä¸ªä»»åŠ¡çš„è¯¦ç»†åˆ†æ
    for task_name in sorted(tasks.keys()):
        task_data = tasks[task_name]
        task_correct = sum(1 for item in task_data if item['is_correct'])
        task_total = len(task_data)
        task_accuracy = task_correct / task_total * 100
        
        print(f"\n" + "=" * 60)
        print(f"ğŸ“‹ {task_name} ä»»åŠ¡åˆ†æ")
        print("=" * 60)
        
        print(f"\nåŸºæœ¬ç»Ÿè®¡:")
        print(f"   å‡†ç¡®ç‡: {task_accuracy:.2f}% ({task_correct}/{task_total})")
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        confidence_stats = defaultdict(lambda: {'correct': 0, 'total': 0})
        for item in task_data:
            conf = item['confidence']
            confidence_stats[conf]['total'] += 1
            if item['is_correct']:
                confidence_stats[conf]['correct'] += 1
        
        print(f"\nç½®ä¿¡åº¦åˆ†å¸ƒ:")
        for conf in sorted(confidence_stats.keys()):
            stats = confidence_stats[conf]
            acc = stats['correct'] / stats['total'] * 100 if stats['total'] > 0 else 0
            pct = stats['total'] / task_total * 100
            print(f"   ç½®ä¿¡åº¦{conf}: {stats['correct']:3d}/{stats['total']:3d} = {acc:5.1f}% (å æ¯”: {pct:4.1f}%)")
        
        # æ¦‚ç‡åˆ†æ
        high_prob = [item for item in task_data if max(item['probabilities'].values()) > 0.8]
        med_prob = [item for item in task_data if 0.6 <= max(item['probabilities'].values()) <= 0.8]
        low_prob = [item for item in task_data if max(item['probabilities'].values()) < 0.6]
        
        print(f"\næ¦‚ç‡åˆ†æ:")
        if high_prob:
            high_correct = sum(1 for item in high_prob if item['is_correct'])
            print(f"   é«˜æ¦‚ç‡(>0.8): {high_correct:3d}/{len(high_prob):3d} = {high_correct/len(high_prob)*100:5.1f}%")
        
        if med_prob:
            med_correct = sum(1 for item in med_prob if item['is_correct'])
            print(f"   ä¸­æ¦‚ç‡(0.6-0.8): {med_correct:3d}/{len(med_prob):3d} = {med_correct/len(med_prob)*100:5.1f}%")
        
        if low_prob:
            low_correct = sum(1 for item in low_prob if item['is_correct'])
            print(f"   ä½æ¦‚ç‡(<0.6): {low_correct:3d}/{len(low_prob):3d} = {low_correct/len(low_prob)*100:5.1f}%")
        
        # å»¶è¿Ÿåˆ†æ
        stage1_latencies = [item['stage1_latency'] for item in task_data if item['stage1_latency'] > 0]
        stage2_latencies = [item['stage2_latency'] for item in task_data if item['stage2_latency'] > 0]
        
        if stage1_latencies:
            avg_stage1 = sum(stage1_latencies) / len(stage1_latencies)
            print(f"\nå»¶è¿Ÿåˆ†æ:")
            print(f"   Stage1å¹³å‡: {avg_stage1:.1f}ms")
        
        if stage2_latencies:
            avg_stage2 = sum(stage2_latencies) / len(stage2_latencies)
            print(f"   Stage2å¹³å‡: {avg_stage2:.1f}ms")
        
        # é”™è¯¯æ¡ˆä¾‹åˆ†æ
        incorrect_items = [item for item in task_data if not item['is_correct']]
        if incorrect_items:
            print(f"\né”™è¯¯åˆ†æ:")
            print(f"   é”™è¯¯æ€»æ•°: {len(incorrect_items)}")
            
            # æŒ‰ç½®ä¿¡åº¦åˆ†ç»„é”™è¯¯
            error_by_conf = defaultdict(int)
            for item in incorrect_items:
                error_by_conf[item['confidence']] += 1
            
            print(f"   é”™è¯¯åˆ†å¸ƒ:")
            for conf in sorted(error_by_conf.keys()):
                print(f"     ç½®ä¿¡åº¦{conf}: {error_by_conf[conf]}ä¸ªé”™è¯¯")
            
            # é«˜ç½®ä¿¡åº¦é”™è¯¯ï¼ˆå¯èƒ½æ˜¯æ¨¡å‹è¿‡åº¦è‡ªä¿¡ï¼‰
            high_conf_errors = [item for item in incorrect_items if item['confidence'] >= 4]
            if high_conf_errors:
                print(f"   âš ï¸  é«˜ç½®ä¿¡åº¦é”™è¯¯: {len(high_conf_errors)}ä¸ª (æ¨¡å‹è¿‡åº¦è‡ªä¿¡)")
    
    # ä»»åŠ¡é—´æ¯”è¾ƒ
    print(f"\n" + "=" * 60)
    print(f"ğŸ”„ ä»»åŠ¡é—´æ¯”è¾ƒ")
    print("=" * 60)
    
    task_accuracies = {}
    for task_name, task_data in tasks.items():
        task_correct = sum(1 for item in task_data if item['is_correct'])
        task_total = len(task_data)
        task_accuracies[task_name] = task_correct / task_total * 100
    
    # æ’åºä»»åŠ¡
    sorted_tasks = sorted(task_accuracies.items(), key=lambda x: x[1], reverse=True)
    
    print(f"\nä»»åŠ¡éš¾åº¦æ’åº (å‡†ç¡®ç‡ä»é«˜åˆ°ä½):")
    for i, (task, acc) in enumerate(sorted_tasks, 1):
        difficulty = "ç®€å•" if acc > 80 else "ä¸­ç­‰" if acc > 60 else "å›°éš¾"
        print(f"   {i}. {task:8s}: {acc:5.2f}% ({difficulty})")
    
    # ç½®ä¿¡åº¦æ ¡å‡†å¯¹æ¯”
    print(f"\nç½®ä¿¡åº¦æ ¡å‡†å¯¹æ¯”:")
    for task_name, task_data in sorted(tasks.items()):
        print(f"\n{task_name}:")
        conf_stats = defaultdict(lambda: {'correct': 0, 'total': 0})
        for item in task_data:
            conf = item['confidence']
            conf_stats[conf]['total'] += 1
            if item['is_correct']:
                conf_stats[conf]['correct'] += 1
        
        for conf in sorted(conf_stats.keys()):
            stats = conf_stats[conf]
            if stats['total'] > 0:
                accuracy = stats['correct'] / stats['total']
                ideal = conf / 5.0
                error = abs(accuracy - ideal)
                print(f"   ç½®ä¿¡åº¦{conf}: å®é™…={accuracy:.3f}, ç†æƒ³={ideal:.3f}, è¯¯å·®={error:.3f}")
    
    print(f"\n" + "=" * 80)
    print(f"ğŸ’¡ å…³é”®å‘ç°:")
    print("=" * 80)
    
    best_task = sorted_tasks[0][0]
    worst_task = sorted_tasks[-1][0]
    print(f"âœ… æœ€æ“…é•¿ä»»åŠ¡: {best_task} ({sorted_tasks[0][1]:.1f}%)")
    print(f"âŒ æœ€å›°éš¾ä»»åŠ¡: {worst_task} ({sorted_tasks[-1][1]:.1f}%)")
    
    accuracy_gap = sorted_tasks[0][1] - sorted_tasks[-1][1]
    print(f"ğŸ“Š ä»»åŠ¡é—´æ€§èƒ½å·®è·: {accuracy_gap:.1f}ä¸ªç™¾åˆ†ç‚¹")
    
    # æ€»ä½“æ ¡å‡†è´¨é‡
    overall_conf_5_items = [item for item in data if item['confidence'] == 5]
    if overall_conf_5_items:
        conf_5_accuracy = sum(1 for item in overall_conf_5_items if item['is_correct']) / len(overall_conf_5_items)
        print(f"ğŸ¯ é«˜ç½®ä¿¡åº¦(5)æ ¡å‡†: å®é™…{conf_5_accuracy:.3f} vs ç†æƒ³1.000 (è¯¯å·®{abs(conf_5_accuracy-1.0):.3f})")
    
    print(f"âš¡ æ•´ä½“æ¨ç†æ•ˆç‡: å¹³å‡{227.2:.1f}msæ¯é¢˜")

if __name__ == "__main__":
    detailed_task_analysis()
