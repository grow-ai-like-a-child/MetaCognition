#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æInternVL3-38Bæ¯ç±»ä»»åŠ¡çš„ç½®ä¿¡åº¦åˆ†å¸ƒ
"""

import json
from collections import defaultdict
from pathlib import Path

def analyze_confidence_by_task():
    """
    åˆ†ææ¯ç±»ä»»åŠ¡çš„ç½®ä¿¡åº¦åˆ†å¸ƒ
    """
    # åŠ è½½æ­£ç¡®æ¯”è¾ƒç»“æœ
    comparison_file = Path("internvl3_correct_comparison.json")
    with open(comparison_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print("=" * 80)
    print("InternVL3-38B æ¯ç±»ä»»åŠ¡ç½®ä¿¡åº¦åˆ†å¸ƒåˆ†æ")
    print("=" * 80)
    
    # æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„
    task_groups = defaultdict(list)
    for item in data:
        task = item.get('task', 'Unknown')
        task_groups[task].append(item)
    
    print(f"\nğŸ“Š ä»»åŠ¡ç±»å‹æ¦‚è§ˆ:")
    print("-" * 60)
    for task in sorted(task_groups.keys()):
        count = len(task_groups[task])
        percentage = count / len(data) * 100
        print(f"   {task:12s}: {count:4d} é¢˜ ({percentage:5.1f}%)")
    
    # åˆ†ææ¯ä¸ªä»»åŠ¡çš„ç½®ä¿¡åº¦åˆ†å¸ƒ
    for task in sorted(task_groups.keys()):
        task_data = task_groups[task]
        
        print(f"\nğŸ” {task} ä»»åŠ¡è¯¦ç»†åˆ†æ:")
        print("=" * 60)
        
        # åŸºæœ¬ç»Ÿè®¡
        total = len(task_data)
        correct = sum(1 for item in task_data if item['is_correct'])
        accuracy = correct / total if total > 0 else 0
        
        print(f"   æ€»é¢˜ç›®æ•°: {total}")
        print(f"   æ­£ç¡®ç­”æ¡ˆæ•°: {correct}")
        print(f"   å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        print(f"\n   ç½®ä¿¡åº¦åˆ†å¸ƒ:")
        print("   " + "-" * 50)
        
        confidence_dist = defaultdict(int)
        confidence_correct = defaultdict(int)
        
        for item in task_data:
            conf = item['confidence']
            confidence_dist[conf] += 1
            if item['is_correct']:
                confidence_correct[conf] += 1
        
        for conf in sorted(confidence_dist.keys()):
            count = confidence_dist[conf]
            correct_count = confidence_correct[conf]
            percentage = count / total * 100
            accuracy_at_conf = correct_count / count if count > 0 else 0
            
            print(f"     ç½®ä¿¡åº¦ {conf}: {count:4d} é¢˜ ({percentage:5.1f}%) | æ­£ç¡®: {correct_count:4d} | å‡†ç¡®ç‡: {accuracy_at_conf:.4f}")
        
        # ç½®ä¿¡åº¦ä¸å‡†ç¡®ç‡å…³ç³»
        print(f"\n   ç½®ä¿¡åº¦ä¸å‡†ç¡®ç‡å…³ç³»:")
        print("   " + "-" * 50)
        
        for conf in sorted(confidence_dist.keys()):
            count = confidence_dist[conf]
            correct_count = confidence_correct[conf]
            accuracy_at_conf = correct_count / count if count > 0 else 0
            calibration_error = abs(accuracy_at_conf - (conf / 5.0))
            
            print(f"     ç½®ä¿¡åº¦ {conf}: å‡†ç¡®ç‡={accuracy_at_conf:.3f}, ç†æƒ³å€¼={conf/5.0:.3f}, æ ¡å‡†è¯¯å·®={calibration_error:.3f}")
        
        # é¢„æµ‹åˆ†å¸ƒ
        print(f"\n   é¢„æµ‹åˆ†å¸ƒ:")
        print("   " + "-" * 30)
        
        predicted_dist = defaultdict(int)
        for item in task_data:
            predicted = item['predicted_choice']
            predicted_dist[predicted] += 1
        
        for choice in ['A', 'B']:
            count = predicted_dist[choice]
            percentage = count / total * 100
            print(f"     é¢„æµ‹{choice}: {count:4d} é¢˜ ({percentage:5.1f}%)")
        
        # é”™è¯¯æ¡ˆä¾‹åˆ†æ
        wrong_cases = [item for item in task_data if not item['is_correct']]
        if wrong_cases:
            print(f"\n   é”™è¯¯æ¡ˆä¾‹åˆ†æ (å…±{len(wrong_cases)}ä¸ªé”™è¯¯):")
            print("   " + "-" * 50)
            
            # æŒ‰ç½®ä¿¡åº¦åˆ†ç»„çš„é”™è¯¯
            wrong_by_confidence = defaultdict(int)
            for item in wrong_cases:
                wrong_by_confidence[item['confidence']] += 1
            
            for conf in sorted(wrong_by_confidence.keys()):
                count = wrong_by_confidence[conf]
                percentage = count / len(wrong_cases) * 100
                print(f"     ç½®ä¿¡åº¦ {conf}: {count:4d} ä¸ªé”™è¯¯ ({percentage:5.1f}%)")
            
            # é«˜ç½®ä¿¡åº¦ä½†é”™è¯¯çš„æ¡ˆä¾‹
            high_conf_wrong = [item for item in wrong_cases if item['confidence'] >= 4]
            if high_conf_wrong:
                print(f"\n     é«˜ç½®ä¿¡åº¦(â‰¥4)ä½†é”™è¯¯: {len(high_conf_wrong)} ä¸ª")
                
                # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
                print(f"     ç¤ºä¾‹é«˜ç½®ä¿¡åº¦é”™è¯¯æ¡ˆä¾‹:")
                for i, item in enumerate(high_conf_wrong[:3]):
                    print(f"       {i+1}. {item['qid']}: é¢„æµ‹={item['predicted_choice']}, æ­£ç¡®={item['correct_choice']}, ç½®ä¿¡åº¦={item['confidence']}")
    
    # è·¨ä»»åŠ¡ç½®ä¿¡åº¦å¯¹æ¯”
    print(f"\nğŸ“ˆ è·¨ä»»åŠ¡ç½®ä¿¡åº¦å¯¹æ¯”:")
    print("=" * 80)
    
    print(f"   ä»»åŠ¡ç±»å‹ | ç½®ä¿¡åº¦3 | ç½®ä¿¡åº¦4 | ç½®ä¿¡åº¦5 | å¹³å‡ç½®ä¿¡åº¦")
    print("   " + "-" * 60)
    
    for task in sorted(task_groups.keys()):
        task_data = task_groups[task]
        
        # è®¡ç®—ç½®ä¿¡åº¦åˆ†å¸ƒ
        confidence_dist = defaultdict(int)
        for item in task_data:
            conf = item['confidence']
            confidence_dist[conf] += 1
        
        total = len(task_data)
        conf3_pct = confidence_dist[3] / total * 100 if total > 0 else 0
        conf4_pct = confidence_dist[4] / total * 100 if total > 0 else 0
        conf5_pct = confidence_dist[5] / total * 100 if total > 0 else 0
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        total_conf = sum(item['confidence'] for item in task_data)
        avg_conf = total_conf / total if total > 0 else 0
        
        print(f"   {task:8s} | {conf3_pct:6.1f}% | {conf4_pct:6.1f}% | {conf5_pct:6.1f}% | {avg_conf:8.2f}")
    
    # ä¿å­˜è¯¦ç»†åˆ†æç»“æœ
    analysis_result = {}
    
    for task in sorted(task_groups.keys()):
        task_data = task_groups[task]
        
        # åŸºæœ¬ç»Ÿè®¡
        total = len(task_data)
        correct = sum(1 for item in task_data if item['is_correct'])
        accuracy = correct / total if total > 0 else 0
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        confidence_dist = defaultdict(int)
        confidence_correct = defaultdict(int)
        
        for item in task_data:
            conf = item['confidence']
            confidence_dist[conf] += 1
            if item['is_correct']:
                confidence_correct[conf] += 1
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        total_conf = sum(item['confidence'] for item in task_data)
        avg_conf = total_conf / total if total > 0 else 0
        
        analysis_result[task] = {
            'total_questions': total,
            'correct_answers': correct,
            'accuracy': accuracy,
            'average_confidence': avg_conf,
            'confidence_distribution': dict(confidence_dist),
            'confidence_accuracy': {
                conf: correct_count / confidence_dist[conf] if confidence_dist[conf] > 0 else 0
                for conf, correct_count in confidence_correct.items()
            }
        }
    
    # ä¿å­˜ç»“æœ
    with open('internvl3_confidence_by_task_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(analysis_result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜åˆ°: internvl3_confidence_by_task_analysis.json")

if __name__ == "__main__":
    analyze_confidence_by_task()
