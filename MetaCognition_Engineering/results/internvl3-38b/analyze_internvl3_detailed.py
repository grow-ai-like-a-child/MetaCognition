#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯¦ç»†åˆ†æInternVL3-38Bæ¨¡å‹æ€§èƒ½
"""

import json
from collections import defaultdict
from pathlib import Path

def analyze_detailed_performance():
    """
    è¯¦ç»†åˆ†æInternVL3æ¨¡å‹æ€§èƒ½
    """
    # åŠ è½½æ­£ç¡®æ¯”è¾ƒç»“æœ
    comparison_file = Path("internvl3_correct_comparison.json")
    with open(comparison_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print("=" * 60)
    print("InternVL3-38B è¯¦ç»†æ€§èƒ½åˆ†æ")
    print("=" * 60)
    
    # åŸºæœ¬ç»Ÿè®¡
    total_questions = len(data)
    correct_answers = sum(1 for item in data if item['is_correct'])
    overall_accuracy = correct_answers / total_questions if total_questions > 0 else 0
    
    print(f"\nğŸ“Š æ€»ä½“æ€§èƒ½:")
    print(f"   æ€»é¢˜ç›®æ•°: {total_questions}")
    print(f"   æ­£ç¡®ç­”æ¡ˆæ•°: {correct_answers}")
    print(f"   æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)")
    
    # æŒ‰ç½®ä¿¡åº¦ç»Ÿè®¡
    print(f"\nğŸ“ˆ æŒ‰ç½®ä¿¡åº¦åˆ†ç»„ç»Ÿè®¡:")
    print("-" * 50)
    
    confidence_stats = defaultdict(lambda: {'total': 0, 'correct': 0})
    
    for item in data:
        conf = item['confidence']
        confidence_stats[conf]['total'] += 1
        if item['is_correct']:
            confidence_stats[conf]['correct'] += 1
    
    # æŒ‰ç½®ä¿¡åº¦æ’åº
    for conf in sorted(confidence_stats.keys()):
        stats = confidence_stats[conf]
        accuracy = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
        percentage = stats['total'] / total_questions * 100
        
        print(f"   ç½®ä¿¡åº¦ {conf}: {stats['correct']:4d}/{stats['total']:4d} = {accuracy:.4f} ({accuracy*100:5.2f}%) | å æ¯”: {percentage:5.2f}%")
    
    # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
    print(f"\nğŸ“‹ æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡:")
    print("-" * 50)
    
    task_stats = defaultdict(lambda: {'total': 0, 'correct': 0})
    
    for item in data:
        task = item.get('task', 'Unknown')
        task_stats[task]['total'] += 1
        if item['is_correct']:
            task_stats[task]['correct'] += 1
    
    for task in sorted(task_stats.keys()):
        stats = task_stats[task]
        accuracy = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
        percentage = stats['total'] / total_questions * 100
        
        print(f"   {task:12s}: {stats['correct']:4d}/{stats['total']:4d} = {accuracy:.4f} ({accuracy*100:5.2f}%) | å æ¯”: {percentage:5.2f}%")
    
    # ç½®ä¿¡åº¦æ ¡å‡†åˆ†æ
    print(f"\nğŸ¯ ç½®ä¿¡åº¦æ ¡å‡†åˆ†æ:")
    print("-" * 50)
    
    # è®¡ç®—æ¯ä¸ªç½®ä¿¡åº¦æ°´å¹³çš„å¹³å‡å‡†ç¡®ç‡
    for conf in sorted(confidence_stats.keys()):
        stats = confidence_stats[conf]
        if stats['total'] > 0:
            accuracy = stats['correct'] / stats['total']
            calibration_error = abs(accuracy - (conf / 5.0))  # ç†æƒ³æƒ…å†µä¸‹ï¼Œç½®ä¿¡åº¦/5åº”è¯¥ç­‰äºå‡†ç¡®ç‡
            print(f"   ç½®ä¿¡åº¦ {conf}: å‡†ç¡®ç‡={accuracy:.3f}, ç†æƒ³å€¼={conf/5.0:.3f}, æ ¡å‡†è¯¯å·®={calibration_error:.3f}")
    
    # é¢„æµ‹åˆ†å¸ƒåˆ†æ
    print(f"\nğŸ” é¢„æµ‹åˆ†å¸ƒåˆ†æ:")
    print("-" * 50)
    
    predicted_dist = defaultdict(int)
    correct_dist = defaultdict(lambda: {'A': 0, 'B': 0})
    
    for item in data:
        predicted = item['predicted_choice']
        correct = item['correct_choice']
        predicted_dist[predicted] += 1
        
        if item['is_correct']:
            correct_dist[correct][predicted] += 1
    
    print(f"   é¢„æµ‹A: {predicted_dist['A']} æ¬¡ ({predicted_dist['A']/total_questions*100:.1f}%)")
    print(f"   é¢„æµ‹B: {predicted_dist['B']} æ¬¡ ({predicted_dist['B']/total_questions*100:.1f}%)")
    
    print(f"\n   æ­£ç¡®ç­”æ¡ˆåˆ†å¸ƒ:")
    for correct_choice in ['A', 'B']:
        total_correct = sum(correct_dist[correct_choice].values())
        if total_correct > 0:
            print(f"     æ­£ç¡®ç­”æ¡ˆ{correct_choice}: {total_correct} é¢˜")
            for predicted_choice in ['A', 'B']:
                count = correct_dist[correct_choice][predicted_choice]
                percentage = count / total_correct * 100
                print(f"       é¢„æµ‹{predicted_choice}: {count} é¢˜ ({percentage:.1f}%)")
    
    # ç½®ä¿¡åº¦ä¸å‡†ç¡®ç‡çš„å…³ç³»
    print(f"\nğŸ“Š ç½®ä¿¡åº¦ä¸å‡†ç¡®ç‡å…³ç³»:")
    print("-" * 50)
    
    for conf in sorted(confidence_stats.keys()):
        stats = confidence_stats[conf]
        if stats['total'] > 0:
            accuracy = stats['correct'] / stats['total']
            print(f"   ç½®ä¿¡åº¦ {conf}: {accuracy:.3f} å‡†ç¡®ç‡ (åŸºäº {stats['total']} ä¸ªæ ·æœ¬)")
    
    # é”™è¯¯æ¡ˆä¾‹åˆ†æ
    print(f"\nâŒ é”™è¯¯æ¡ˆä¾‹åˆ†æ:")
    print("-" * 50)
    
    wrong_cases = [item for item in data if not item['is_correct']]
    print(f"   æ€»é”™è¯¯æ•°: {len(wrong_cases)}")
    
    # æŒ‰ç½®ä¿¡åº¦åˆ†ç»„çš„é”™è¯¯
    wrong_by_confidence = defaultdict(int)
    for item in wrong_cases:
        wrong_by_confidence[item['confidence']] += 1
    
    print(f"   æŒ‰ç½®ä¿¡åº¦åˆ†ç»„çš„é”™è¯¯:")
    for conf in sorted(wrong_by_confidence.keys()):
        count = wrong_by_confidence[conf]
        percentage = count / len(wrong_cases) * 100
        print(f"     ç½®ä¿¡åº¦ {conf}: {count} ä¸ªé”™è¯¯ ({percentage:.1f}%)")
    
    # é«˜ç½®ä¿¡åº¦ä½†é”™è¯¯çš„æ¡ˆä¾‹
    high_conf_wrong = [item for item in wrong_cases if item['confidence'] >= 4]
    print(f"   é«˜ç½®ä¿¡åº¦(â‰¥4)ä½†é”™è¯¯: {len(high_conf_wrong)} ä¸ª")
    
    if len(high_conf_wrong) > 0:
        print(f"   ç¤ºä¾‹é«˜ç½®ä¿¡åº¦é”™è¯¯æ¡ˆä¾‹:")
        for i, item in enumerate(high_conf_wrong[:3]):  # æ˜¾ç¤ºå‰3ä¸ª
            print(f"     {i+1}. {item['qid']}: é¢„æµ‹={item['predicted_choice']}, æ­£ç¡®={item['correct_choice']}, ç½®ä¿¡åº¦={item['confidence']}")

if __name__ == "__main__":
    analyze_detailed_performance()
