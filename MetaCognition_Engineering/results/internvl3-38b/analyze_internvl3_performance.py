#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æInternVL3-38Bæ¨¡å‹æ€§èƒ½ï¼šæ­£ç¡®ç‡å’Œç½®ä¿¡åº¦ç»Ÿè®¡
"""

import json
import re
from collections import defaultdict
from pathlib import Path

def load_ground_truth():
    """
    åŠ è½½çœŸå®æ ‡ç­¾æ•°æ®
    """
    ground_truth = {}
    
    # åŠ è½½é—®é¢˜æ–‡ä»¶è·å–çœŸå®æ ‡ç­¾
    questions_file = Path("../../data/prompt/questions_two_stage_concise.jsonl")
    with open(questions_file, 'r', encoding='utf-8') as f:
        for line in f:
            question = json.loads(line.strip())
            if question.get('stage') == 1:  # åªå¤„ç†stage1é—®é¢˜
                qid = question['qid']
                original_qid = question['original_qid']
                
                # ä»contextä¸­æå–çœŸå®ç­”æ¡ˆ
                context = question.get('context', {})
                derived = context.get('derived', {})
                gt = derived.get('gt', {})
                
                if 'more_symbol' in gt:
                    more_symbol = gt['more_symbol']
                    if more_symbol == 'symA':
                        ground_truth[original_qid] = 'A'
                    elif more_symbol == 'symB':
                        ground_truth[original_qid] = 'B'
    
    return ground_truth

def analyze_model_performance():
    """
    åˆ†æInternVL3æ¨¡å‹æ€§èƒ½
    """
    # åŠ è½½çœŸå®æ ‡ç­¾
    ground_truth = load_ground_truth()
    print(f"åŠ è½½äº† {len(ground_truth)} ä¸ªçœŸå®æ ‡ç­¾")
    
    # åŠ è½½æ¨¡å‹ç»“æœ
    results_file = Path("internvl3_complete_results.jsonl")
    results = []
    
    with open(results_file, 'r', encoding='utf-8') as f:
        for line in f:
            results.append(json.loads(line.strip()))
    
    print(f"åŠ è½½äº† {len(results)} ä¸ªæ¨¡å‹ç»“æœ")
    
    # æŒ‰original_qidåˆ†ç»„ï¼Œåªåˆ†æstage1ç»“æœ
    stage1_results = {}
    stage2_results = {}
    
    for result in results:
        original_qid = result['original_qid']
        stage = result['stage']
        
        if stage == 1:
            stage1_results[original_qid] = result
        elif stage == 2:
            stage2_results[original_qid] = result
    
    print(f"æ‰¾åˆ° {len(stage1_results)} ä¸ªStage 1ç»“æœ")
    print(f"æ‰¾åˆ° {len(stage2_results)} ä¸ªStage 2ç»“æœ")
    
    # åˆ†æStage 1æ­£ç¡®ç‡
    print("\n" + "=" * 60)
    print("InternVL3-38B Stage 1 æ€§èƒ½åˆ†æ")
    print("=" * 60)
    
    correct_count = 0
    total_count = 0
    confidence_stats = defaultdict(lambda: {'total': 0, 'correct': 0})
    
    for original_qid, result in stage1_results.items():
        if original_qid not in ground_truth:
            continue
            
        total_count += 1
        predicted = result['choice']
        true_answer = ground_truth[original_qid]
        
        # æ£€æŸ¥æ˜¯å¦æ­£ç¡®
        is_correct = (predicted == true_answer)
        if is_correct:
            correct_count += 1
        
        # è·å–å¯¹åº”çš„Stage 2ç½®ä¿¡åº¦
        if original_qid in stage2_results:
            confidence = stage2_results[original_qid]['confidence']
            confidence_stats[confidence]['total'] += 1
            if is_correct:
                confidence_stats[confidence]['correct'] += 1
    
    # è®¡ç®—æ€»ä½“å‡†ç¡®ç‡
    overall_accuracy = correct_count / total_count if total_count > 0 else 0
    
    print(f"\nğŸ“Š Stage 1 æ€»ä½“æ€§èƒ½:")
    print(f"   æ€»é¢˜ç›®æ•°: {total_count}")
    print(f"   æ­£ç¡®ç­”æ¡ˆæ•°: {correct_count}")
    print(f"   æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)")
    
    # æŒ‰ç½®ä¿¡åº¦ç»Ÿè®¡
    print(f"\nğŸ“ˆ æŒ‰ç½®ä¿¡åº¦åˆ†ç»„ç»Ÿè®¡:")
    print("-" * 50)
    
    for conf in sorted(confidence_stats.keys()):
        stats = confidence_stats[conf]
        accuracy = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
        percentage = stats['total'] / total_count * 100
        
        print(f"   ç½®ä¿¡åº¦ {conf}: {stats['correct']:4d}/{stats['total']:4d} = {accuracy:.4f} ({accuracy*100:5.2f}%) | å æ¯”: {percentage:5.2f}%")
    
    # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
    print(f"\nğŸ“‹ æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡:")
    print("-" * 50)
    
    task_stats = defaultdict(lambda: {'total': 0, 'correct': 0})
    
    for original_qid, result in stage1_results.items():
        if original_qid not in ground_truth:
            continue
            
        task = result.get('task', 'Unknown')
        predicted = result['choice']
        true_answer = ground_truth[original_qid]
        is_correct = (predicted == true_answer)
        
        task_stats[task]['total'] += 1
        if is_correct:
            task_stats[task]['correct'] += 1
    
    for task in sorted(task_stats.keys()):
        stats = task_stats[task]
        accuracy = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
        percentage = stats['total'] / total_count * 100
        
        print(f"   {task:12s}: {stats['correct']:4d}/{stats['total']:4d} = {accuracy:.4f} ({accuracy*100:5.2f}%) | å æ¯”: {percentage:5.2f}%")
    
    # ç½®ä¿¡åº¦æ ¡å‡†åˆ†æ
    print(f"\nğŸ¯ ç½®ä¿¡åº¦æ ¡å‡†åˆ†æ:")
    print("-" * 50)
    
    # è®¡ç®—æ¯ä¸ªç½®ä¿¡åº¦æ°´å¹³çš„å¹³å‡å‡†ç¡®ç‡
    for conf in sorted(confidence_stats.keys()):
        stats = confidence_stats[conf]
        if stats['total'] > 0:
            accuracy = stats['correct'] / stats['total']
            calibration_error = abs(accuracy - (conf / 5.0))  # ç†æƒ³æƒ…å†µä¸‹ï¼Œç½®ä¿¡åº¦/5åº”è¯¥ç­‰äºå‡†ç¡®ç‡
            print(f"   ç½®ä¿¡åº¦ {conf}: å‡†ç¡®ç‡={accuracy:.3f}, ç†æƒ³å€¼={conf/5.0:.3f}, æ ¡å‡†è¯¯å·®={calibration_error:.3f}")
    
    # ä¿å­˜è¯¦ç»†æ¯”è¾ƒç»“æœ
    comparison_data = []
    for original_qid, result in stage1_results.items():
        if original_qid not in ground_truth:
            continue
            
        predicted = result['choice']
        true_answer = ground_truth[original_qid]
        is_correct = (predicted == true_answer)
        
        confidence = 0
        if original_qid in stage2_results:
            confidence = stage2_results[original_qid]['confidence']
        
        comparison_data.append({
            'original_qid': original_qid,
            'task': result.get('task', 'Unknown'),
            'predicted': predicted,
            'true_answer': true_answer,
            'is_correct': is_correct,
            'confidence': confidence,
            'probabilities': result.get('probabilities', {}),
            'latency_ms': result.get('latency_ms', 0)
        })
    
    # ä¿å­˜æ¯”è¾ƒç»“æœ
    with open('internvl3_comparison_with_ground_truth.json', 'w', encoding='utf-8') as f:
        json.dump(comparison_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è¯¦ç»†æ¯”è¾ƒç»“æœå·²ä¿å­˜åˆ°: internvl3_comparison_with_ground_truth.json")
    print(f"   åŒ…å« {len(comparison_data)} ä¸ªé¢˜ç›®çš„è¯¦ç»†åˆ†æ")

if __name__ == "__main__":
    analyze_model_performance()
