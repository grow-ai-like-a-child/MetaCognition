#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æžQwen2.5-VL-72Bæ¨¡åž‹æ€§èƒ½ï¼šæ­£ç¡®çŽ‡å’Œç½®ä¿¡åº¦ç»Ÿè®¡
"""

import json
from collections import defaultdict

def analyze_model_performance():
    """
    åˆ†æžæ¨¡åž‹æ€§èƒ½
    """
    # åŠ è½½æ•°æ®
    with open('/home/ubuntu/MetaCognition/MetaCognition_Engineering/data/results/qwen72b/qwen72b_simple_comparison.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print("=" * 60)
    print("Qwen2.5-VL-72B æ¨¡åž‹æ€§èƒ½åˆ†æž")
    print("=" * 60)
    
    # åŸºæœ¬ç»Ÿè®¡
    total_questions = len(data)
    correct_answers = sum(1 for item in data if item['is_correct'])
    overall_accuracy = correct_answers / total_questions if total_questions > 0 else 0
    
    print(f"\nðŸ“Š æ€»ä½“æ€§èƒ½:")
    print(f"   æ€»é¢˜ç›®æ•°: {total_questions}")
    print(f"   æ­£ç¡®ç­”æ¡ˆæ•°: {correct_answers}")
    print(f"   æ•´ä½“å‡†ç¡®çŽ‡: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)")
    
    # æŒ‰ç½®ä¿¡åº¦ç»Ÿè®¡
    print(f"\nðŸ“ˆ æŒ‰ç½®ä¿¡åº¦åˆ†ç»„ç»Ÿè®¡:")
    print("-" * 50)
    
    confidence_stats = defaultdict(lambda: {'total': 0, 'correct': 0})
    
    for item in data:
        conf = item['confidence']
        confidence_stats[conf]['total'] += 1
        if item['is_correct']:
            confidence_stats[conf]['correct'] += 1
    
    # æŒ‰ç½®ä¿¡åº¦æŽ’åº
    for conf in sorted(confidence_stats.keys()):
        stats = confidence_stats[conf]
        accuracy = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
        percentage = stats['total'] / total_questions * 100
        
        print(f"   ç½®ä¿¡åº¦ {conf}: {stats['correct']:4d}/{stats['total']:4d} = {accuracy:.4f} ({accuracy*100:5.2f}%) | å æ¯”: {percentage:5.2f}%")
    
    # æŒ‰ä»»åŠ¡ç±»åž‹ç»Ÿè®¡
    print(f"\nðŸŽ¯ æŒ‰ä»»åŠ¡ç±»åž‹ç»Ÿè®¡:")
    print("-" * 50)
    
    task_stats = defaultdict(lambda: {'total': 0, 'correct': 0})
    
    for item in data:
        # ä»Žqidä¸­æå–ä»»åŠ¡ç±»åž‹
        task_type = item['qid'].split('-')[0]
        task_stats[task_type]['total'] += 1
        if item['is_correct']:
            task_stats[task_type]['correct'] += 1
    
    for task in sorted(task_stats.keys()):
        stats = task_stats[task]
        accuracy = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
        percentage = stats['total'] / total_questions * 100
        
        print(f"   {task:8s}: {stats['correct']:4d}/{stats['total']:4d} = {accuracy:.4f} ({accuracy*100:5.2f}%) | å æ¯”: {percentage:5.2f}%")
    
    # ç½®ä¿¡åº¦æ ¡å‡†åˆ†æž
    print(f"\nðŸŽ¯ ç½®ä¿¡åº¦æ ¡å‡†åˆ†æž:")
    print("-" * 50)
    
    # é«˜ç½®ä¿¡åº¦(4-5) vs ä½Žç½®ä¿¡åº¦(1-3)çš„å‡†ç¡®çŽ‡å¯¹æ¯”
    high_conf_correct = sum(1 for item in data if item['confidence'] >= 4 and item['is_correct'])
    high_conf_total = sum(1 for item in data if item['confidence'] >= 4)
    low_conf_correct = sum(1 for item in data if item['confidence'] < 4 and item['is_correct'])
    low_conf_total = sum(1 for item in data if item['confidence'] < 4)
    
    high_conf_accuracy = high_conf_correct / high_conf_total if high_conf_total > 0 else 0
    low_conf_accuracy = low_conf_correct / low_conf_total if low_conf_total > 0 else 0
    
    print(f"   é«˜ç½®ä¿¡åº¦(4-5): {high_conf_correct:4d}/{high_conf_total:4d} = {high_conf_accuracy:.4f} ({high_conf_accuracy*100:5.2f}%)")
    print(f"   ä½Žç½®ä¿¡åº¦(1-3): {low_conf_correct:4d}/{low_conf_total:4d} = {low_conf_accuracy:.4f} ({low_conf_accuracy*100:5.2f}%)")
    
    # ç½®ä¿¡åº¦åˆ†å¸ƒ
    print(f"\nðŸ“Š ç½®ä¿¡åº¦åˆ†å¸ƒ:")
    print("-" * 30)
    for conf in sorted(confidence_stats.keys()):
        count = confidence_stats[conf]['total']
        percentage = count / total_questions * 100
        bar = "â–ˆ" * int(percentage / 2)  # æ¯2%ä¸€ä¸ªæ–¹å—
        print(f"   {conf}: {count:4d} ({percentage:5.2f}%) {bar}")
    
    # å…ƒè®¤çŸ¥èƒ½åŠ›åˆ†æž
    print(f"\nðŸ§  å…ƒè®¤çŸ¥èƒ½åŠ›åˆ†æž:")
    print("-" * 50)
    
    # è®¡ç®—ç½®ä¿¡åº¦ä¸Žå‡†ç¡®çŽ‡çš„ç›¸å…³æ€§
    conf_acc_correlation = []
    for conf in sorted(confidence_stats.keys()):
        stats = confidence_stats[conf]
        if stats['total'] > 0:
            accuracy = stats['correct'] / stats['total']
            conf_acc_correlation.append((conf, accuracy, stats['total']))
    
    print("   ç½®ä¿¡åº¦ vs å‡†ç¡®çŽ‡:")
    for conf, acc, count in conf_acc_correlation:
        print(f"   {conf}: {acc:.4f} (n={count})")
    
    # åˆ¤æ–­æ¨¡åž‹æ˜¯å¦è¿‡åº¦è‡ªä¿¡
    if len(conf_acc_correlation) >= 2:
        high_conf_acc = conf_acc_correlation[-1][1]  # æœ€é«˜ç½®ä¿¡åº¦çš„å‡†ç¡®çŽ‡
        low_conf_acc = conf_acc_correlation[0][1]    # æœ€ä½Žç½®ä¿¡åº¦çš„å‡†ç¡®çŽ‡
        
        if high_conf_acc < low_conf_acc:
            print(f"\n   âš ï¸  æ¨¡åž‹å¯èƒ½è¿‡åº¦è‡ªä¿¡ï¼šé«˜ç½®ä¿¡åº¦å‡†ç¡®çŽ‡({high_conf_acc:.4f}) < ä½Žç½®ä¿¡åº¦å‡†ç¡®çŽ‡({low_conf_acc:.4f})")
        else:
            print(f"\n   âœ… æ¨¡åž‹ç½®ä¿¡åº¦æ ¡å‡†è‰¯å¥½ï¼šé«˜ç½®ä¿¡åº¦å‡†ç¡®çŽ‡({high_conf_acc:.4f}) > ä½Žç½®ä¿¡åº¦å‡†ç¡®çŽ‡({low_conf_acc:.4f})")
    
    print("\n" + "=" * 60)

if __name__ == "__main__":
    analyze_model_performance()
