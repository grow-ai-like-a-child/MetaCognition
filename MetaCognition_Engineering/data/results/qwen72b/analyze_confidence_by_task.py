#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æQwen2.5-VL-72Bæ¨¡å‹åœ¨ä¸‰ç§ä»»åŠ¡ç±»å‹ä¸­çš„ç½®ä¿¡åº¦åˆ†å¸ƒ
"""

import json
from collections import defaultdict

def analyze_confidence_by_task():
    """
    åˆ†æä¸åŒä»»åŠ¡ç±»å‹çš„ç½®ä¿¡åº¦åˆ†å¸ƒ
    """
    # åŠ è½½æ•°æ®
    with open('/home/ubuntu/MetaCognition/MetaCognition_Engineering/data/results/qwen72b/qwen72b_simple_comparison.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print("=" * 80)
    print("Qwen2.5-VL-72B æ¨¡å‹ä¸‰ç§ä»»åŠ¡ç±»å‹çš„ç½®ä¿¡åº¦åˆ†å¸ƒåˆ†æ")
    print("=" * 80)
    
    # æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„
    task_data = defaultdict(list)
    for item in data:
        task_type = item['qid'].split('-')[0]
        task_data[task_type].append(item)
    
    # åˆ†ææ¯ç§ä»»åŠ¡ç±»å‹
    for task_type in ['GRID', 'COL', 'GAB']:
        if task_type not in task_data:
            continue
            
        task_items = task_data[task_type]
        total = len(task_items)
        correct = sum(1 for item in task_items if item['is_correct'])
        accuracy = correct / total if total > 0 else 0
        
        print(f"\nğŸ¯ {task_type} ä»»åŠ¡åˆ†æ:")
        print("-" * 60)
        print(f"   æ€»é¢˜ç›®æ•°: {total}")
        print(f"   æ­£ç¡®ç­”æ¡ˆæ•°: {correct}")
        print(f"   å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        print(f"\nğŸ“Š {task_type} ç½®ä¿¡åº¦åˆ†å¸ƒ:")
        print("-" * 40)
        
        confidence_stats = defaultdict(lambda: {'total': 0, 'correct': 0})
        
        for item in task_items:
            conf = item['confidence']
            confidence_stats[conf]['total'] += 1
            if item['is_correct']:
                confidence_stats[conf]['correct'] += 1
        
        # æŒ‰ç½®ä¿¡åº¦æ’åºæ˜¾ç¤º
        for conf in sorted(confidence_stats.keys()):
            stats = confidence_stats[conf]
            accuracy_at_conf = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
            percentage = stats['total'] / total * 100
            
            # åˆ›å»ºå¯è§†åŒ–æ¡å½¢å›¾
            bar_length = int(percentage / 2)  # æ¯2%ä¸€ä¸ªæ–¹å—
            bar = "â–ˆ" * bar_length
            
            print(f"   ç½®ä¿¡åº¦ {conf}: {stats['correct']:3d}/{stats['total']:3d} = {accuracy_at_conf:.4f} ({accuracy_at_conf*100:5.2f}%) | å æ¯”: {percentage:5.2f}% {bar}")
        
        # ç½®ä¿¡åº¦æ ¡å‡†åˆ†æ
        print(f"\nğŸ§  {task_type} ç½®ä¿¡åº¦æ ¡å‡†:")
        print("-" * 30)
        
        # è®¡ç®—ä¸åŒç½®ä¿¡åº¦æ°´å¹³çš„å‡†ç¡®ç‡
        conf_acc_pairs = []
        for conf in sorted(confidence_stats.keys()):
            stats = confidence_stats[conf]
            if stats['total'] > 0:
                accuracy_at_conf = stats['correct'] / stats['total']
                conf_acc_pairs.append((conf, accuracy_at_conf, stats['total']))
        
        if len(conf_acc_pairs) >= 2:
            # æ£€æŸ¥ç½®ä¿¡åº¦æ ¡å‡†
            high_conf_acc = conf_acc_pairs[-1][1]  # æœ€é«˜ç½®ä¿¡åº¦çš„å‡†ç¡®ç‡
            low_conf_acc = conf_acc_pairs[0][1]    # æœ€ä½ç½®ä¿¡åº¦çš„å‡†ç¡®ç‡
            
            print(f"   æœ€é«˜ç½®ä¿¡åº¦å‡†ç¡®ç‡: {high_conf_acc:.4f}")
            print(f"   æœ€ä½ç½®ä¿¡åº¦å‡†ç¡®ç‡: {low_conf_acc:.4f}")
            
            if high_conf_acc > low_conf_acc:
                print(f"   âœ… ç½®ä¿¡åº¦æ ¡å‡†è‰¯å¥½")
            elif high_conf_acc < low_conf_acc:
                print(f"   âš ï¸  ç½®ä¿¡åº¦æ ¡å‡†ä¸ä½³ï¼ˆè¿‡åº¦è‡ªä¿¡ï¼‰")
            else:
                print(f"   â– ç½®ä¿¡åº¦æ ¡å‡†ä¸­æ€§")
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒç»Ÿè®¡
        print(f"\nğŸ“ˆ {task_type} ç½®ä¿¡åº¦ç»Ÿè®¡:")
        print("-" * 30)
        
        conf_values = [item['confidence'] for item in task_items]
        if conf_values:
            avg_confidence = sum(conf_values) / len(conf_values)
            min_confidence = min(conf_values)
            max_confidence = max(conf_values)
            
            print(f"   å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.2f}")
            print(f"   æœ€ä½ç½®ä¿¡åº¦: {min_confidence}")
            print(f"   æœ€é«˜ç½®ä¿¡åº¦: {max_confidence}")
            
            # ç½®ä¿¡åº¦åˆ†å¸ƒç™¾åˆ†æ¯”
            conf_distribution = defaultdict(int)
            for conf in conf_values:
                conf_distribution[conf] += 1
            
            print(f"   ç½®ä¿¡åº¦åˆ†å¸ƒ:")
            for conf in sorted(conf_distribution.keys()):
                count = conf_distribution[conf]
                percentage = count / total * 100
                print(f"     {conf}: {count:3d} ({percentage:5.2f}%)")
    
    # è·¨ä»»åŠ¡ç±»å‹æ¯”è¾ƒ
    print(f"\nğŸ”„ è·¨ä»»åŠ¡ç±»å‹æ¯”è¾ƒ:")
    print("-" * 60)
    
    comparison_data = []
    for task_type in ['GRID', 'COL', 'GAB']:
        if task_type in task_data:
            task_items = task_data[task_type]
            total = len(task_items)
            correct = sum(1 for item in task_items if item['is_correct'])
            accuracy = correct / total if total > 0 else 0
            
            # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
            conf_values = [item['confidence'] for item in task_items]
            avg_confidence = sum(conf_values) / len(conf_values) if conf_values else 0
            
            # è®¡ç®—ç½®ä¿¡åº¦5çš„å æ¯”
            conf_5_count = sum(1 for conf in conf_values if conf == 5)
            conf_5_percentage = conf_5_count / total * 100 if total > 0 else 0
            
            comparison_data.append({
                'task': task_type,
                'accuracy': accuracy,
                'avg_confidence': avg_confidence,
                'conf_5_percentage': conf_5_percentage,
                'total': total
            })
    
    # æ˜¾ç¤ºæ¯”è¾ƒè¡¨æ ¼
    print(f"{'ä»»åŠ¡ç±»å‹':<8} {'å‡†ç¡®ç‡':<10} {'å¹³å‡ç½®ä¿¡åº¦':<12} {'ç½®ä¿¡åº¦5å æ¯”':<12} {'é¢˜ç›®æ•°':<8}")
    print("-" * 60)
    for data in comparison_data:
        print(f"{data['task']:<8} {data['accuracy']:<10.4f} {data['avg_confidence']:<12.2f} {data['conf_5_percentage']:<12.2f} {data['total']:<8}")
    
    # åˆ†æç½®ä¿¡åº¦ä¸å‡†ç¡®ç‡çš„å…³ç³»
    print(f"\nğŸ” ç½®ä¿¡åº¦ä¸å‡†ç¡®ç‡å…³ç³»åˆ†æ:")
    print("-" * 50)
    
    for data in comparison_data:
        task = data['task']
        accuracy = data['accuracy']
        avg_conf = data['avg_confidence']
        conf_5_pct = data['conf_5_percentage']
        
        print(f"{task}:")
        print(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {avg_conf:.2f}")
        print(f"   ç½®ä¿¡åº¦5å æ¯”: {conf_5_pct:.2f}%")
        
        # åˆ¤æ–­æ˜¯å¦è¿‡åº¦è‡ªä¿¡
        if conf_5_pct > 90 and accuracy < 0.6:
            print(f"   âš ï¸  å¯èƒ½è¿‡åº¦è‡ªä¿¡ï¼šé«˜ç½®ä¿¡åº¦å æ¯”({conf_5_pct:.1f}%)ä½†å‡†ç¡®ç‡è¾ƒä½({accuracy:.4f})")
        elif conf_5_pct < 50 and accuracy > 0.8:
            print(f"   âœ… ç½®ä¿¡åº¦é€‚ä¸­ï¼šä½ç½®ä¿¡åº¦å æ¯”({conf_5_pct:.1f}%)ä¸”å‡†ç¡®ç‡é«˜({accuracy:.4f})")
        else:
            print(f"   â– ç½®ä¿¡åº¦ä¸å‡†ç¡®ç‡åŒ¹é…åº¦ä¸€èˆ¬")
        print()
    
    print("=" * 80)

if __name__ == "__main__":
    analyze_confidence_by_task()
